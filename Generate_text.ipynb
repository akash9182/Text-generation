{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# I use an LSTM RNN in tensorflow that learns from wiki text to\n",
    "# generate new text.\n",
    "# The WikiText language modeling dataset is a collection of over 100\n",
    "# million tokens extracted from the set of verified Good and \n",
    "# Featured articles on Wikipedia.\n",
    "import numpy as np\n",
    "import random \n",
    "import tensorflow as tf\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text length in number of characters:  1279333\n",
      "Start of text: \n",
      " \n",
      " = Robert Boulter = \n",
      " \n",
      " Robert Boulter is an English film , television and theatre actor . He had a guest @-@ starring role on the television series The Bill in 2000 . This was followed by a starring role in the play Herons written by Simon Stephens , which was performed in 2001 at the Royal Court Theatre . He had a guest role in the television series Judge John Deed in 2002 . In 2004 Boulter landed a role as \" Craig \" in the episode \" Teddy 's Story \" of the television series The Long Firm ; he starred alongside actors Mark Strong and Derek Jacobi . He was cast in the 2005 theatre productions of the Philip Ridley play Mercury Fur , which was performed at the Drum Theatre in Plymouth and the <unk> Chocolate Factory in London . He was directed by John Tiffany and starred alongside Ben Whishaw , Shane Zaza , Harry Kent , Fraser Ayres , Sophie Stanton and Dominic Hall . \n",
      " In 2006 , Boulter starred alongside Whishaw in the play Citizenship written by Mark Ravenhill . He appeared on a 200\n"
     ]
    }
   ],
   "source": [
    "text = open(\"wikitext-103/wiki.test.tokens\").read()\n",
    "# text = text[:250000]\n",
    "print('text length in number of characters: ', len(text))\n",
    "print('Start of text: ')\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'2', '1', 'a', 's', '3', 'h', 'k'}\n",
      "['2', '1', 'a', 's', '3', 'h', 'k']\n",
      "['1', '2', '3', 'a', 'h', 'k', 's']\n"
     ]
    }
   ],
   "source": [
    "a = \"akash123\"\n",
    "print(set(a))\n",
    "print(list(set(a)))\n",
    "print(sorted(list(set(a))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of characters:  157\n",
      "['\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '^', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '£', '¥', '©', '°', '½', 'Æ', 'É', '×', 'ß', 'à', 'á', 'ã', 'ä', 'å', 'æ', 'ç', 'è', 'é', 'ê', 'í', 'ñ', 'ó', 'ô', 'ö', 'ú', 'ü', 'ć', 'č', 'ī', 'ł', 'Ō', 'ō', 'Š', 'ū', '̍', '͘', 'Π', 'Ω', 'α', 'β', 'ω', 'א', 'ב', 'י', 'ל', 'ר', 'ש', 'ת', 'ग', '्', 'ả', 'ẩ', '‑', '–', '—', '’', '“', '”', '†', '‡', '…', '⁄', '₩', '₱', '→', '−', '♯', '王']\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "char_size = len(chars)\n",
    "print('number of characters: ', char_size)\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every element in **char** array only appears once. So the **text** array contains all the examples, and the **chars** array acts like a features holder, which we then create two dictionaries to map between indexes and characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    " # This helps to map character to a number and vice versa\n",
    "char2id = dict((c,i) for i,c in enumerate(chars))\n",
    "id2char = dict((i,c) for i,c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Given a probability of each character, return a likely character, one-hot encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sample(prediction):\n",
    "    # Samples are uniformly distributed over interval\n",
    "    r = random.uniform(0,1)\n",
    "    s = 0\n",
    "    char_id = len(prediction) - 1 \n",
    "    for i in range(len(prediction)):\n",
    "        s += prediction[i]\n",
    "        # check if probability greater than our randomly generated one\n",
    "        if s >= r:\n",
    "            char_id = i\n",
    "            break\n",
    "    \n",
    "    char_one_hot = np.zeros(shape=[char_size])\n",
    "    char_one_hot[char_id] = 1.0\n",
    "    return char_one_hot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "len_per_section = 50\n",
    "skip = 2\n",
    "\n",
    "sections = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - len_per_section, skip):\n",
    "    sections.append(text[i: i + len_per_section])\n",
    "    next_chars.append(text[i + len_per_section])\n",
    "\n",
    "#Vectorize input and output\n",
    "X = np.zeros((len(sections), len_per_section, char_size))\n",
    "y = np.zeros((len(sections), char_size))\n",
    "for i, section in enumerate(sections):\n",
    "    for j, char in enumerate(section):\n",
    "        X[i, j, char2id[char]] = 1\n",
    "    y[i, char2id[next_chars[i]]] = 1\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data size: 639642\n",
      "approximate steps per epoch: 1249\n"
     ]
    }
   ],
   "source": [
    "#Batch size defines number of samples that going to be propagated through the network.\n",
    "#one epoch = one forward pass and one backward pass of all the training examples\n",
    "#batch size = the number of training examples in one forward/backward pass.\n",
    "#The higher the batch size, the more memory space you'll need.\n",
    "#if you have 1000 training examples, \n",
    "#and your batch size is 500, then it will take 2 iterations to complete 1 epoch.\n",
    "batch_size = 512\n",
    "max_steps = 72000\n",
    "log_every = 3000\n",
    "test_every = 6000\n",
    "hidden_nodes = 1024\n",
    "test_start = \"Let's build something \"\n",
    "checkpoint_directory = 'ckpt'\n",
    "\n",
    "#Create a checkpoint directory\n",
    "if tf.gfile.Exists(checkpoint_directory):\n",
    "    tf.gfile.DeleteRecursively(checkpoint_directory)\n",
    "tf.gfile.MakeDirs(checkpoint_directory)\n",
    "\n",
    "print('training data size:', len(X))\n",
    "print('approximate steps per epoch:', int(len(X)/batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model\n",
    "<img src=\"LSTM.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#build our model time\n",
    "#create computation graph\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    ###########\n",
    "    #Prep\n",
    "    ###########\n",
    "    #Variables and placeholders\n",
    "    global_step = tf.Variable(0)\n",
    "    \n",
    "    data = tf.placeholder(tf.float32, [batch_size, len_per_section, char_size])\n",
    "    labels = tf.placeholder(tf.float32, [batch_size, char_size])\n",
    "    \n",
    "    #Prep LSTM Operation\n",
    "    #Input gate: weights for input, weights for previous output, and bias\n",
    "    w_ii = tf.Variable(tf.truncated_normal([char_size, hidden_nodes], -0.1, 0.1))\n",
    "    w_io = tf.Variable(tf.truncated_normal([hidden_nodes, hidden_nodes], -0.1, 0.1))\n",
    "    b_i = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "    #Forget gate: weights for input, weights for previous output, and bias\n",
    "    w_fi = tf.Variable(tf.truncated_normal([char_size, hidden_nodes], -0.1, 0.1))\n",
    "    w_fo = tf.Variable(tf.truncated_normal([hidden_nodes, hidden_nodes], -0.1, 0.1))\n",
    "    b_f = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "    #Output gate: weights for input, weights for previous output, and bias\n",
    "    w_oi = tf.Variable(tf.truncated_normal([char_size, hidden_nodes], -0.1, 0.1))\n",
    "    w_oo = tf.Variable(tf.truncated_normal([hidden_nodes, hidden_nodes], -0.1, 0.1))\n",
    "    b_o = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "    #Memory cell: weights for input, weights for previous output, and bias\n",
    "    w_ci = tf.Variable(tf.truncated_normal([char_size, hidden_nodes], -0.1, 0.1))\n",
    "    w_co = tf.Variable(tf.truncated_normal([hidden_nodes, hidden_nodes], -0.1, 0.1))\n",
    "    b_c = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "    \n",
    "    #LCTM Cell\n",
    "    #An LSTM RNN (Long Short Term Memory), consists of 3 gates and an internal state, \n",
    "    #This enables the LSTM to capture long-term dependencies. \n",
    "    def lstm(i, o, state):\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, w_ii) + tf.matmul(o, w_io) + b_i)\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, w_fi) + tf.matmul(o, w_fo) + b_f)\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, w_oi) + tf.matmul(o, w_oo) + b_o)\n",
    "        memory_cell = tf.sigmoid(tf.matmul(i, w_ci) + tf.matmul(o, w_co) + b_c)\n",
    "        state = forget_gate * state + input_gate * memory_cell\n",
    "        output = output_gate * tf.tanh(state)\n",
    "        return output, state\n",
    "    \n",
    "    ###########\n",
    "    #Operation\n",
    "    ###########\n",
    "    #LSTM\n",
    "    output = tf.zeros([batch_size, hidden_nodes])\n",
    "    state = tf.zeros([batch_size, hidden_nodes])\n",
    "\n",
    "    for i in range(len_per_section):\n",
    "        output, state = lstm(data[:, i, :], output, state)\n",
    "        if i == 0:\n",
    "            outputs_all_i = output\n",
    "            labels_all_i = data[:, i+1, :]\n",
    "        elif i != len_per_section - 1:\n",
    "            outputs_all_i = tf.concat(0, [outputs_all_i, output])\n",
    "            labels_all_i = tf.concat(0, [labels_all_i, data[:, i+1, :]])\n",
    "        else:\n",
    "            outputs_all_i = tf.concat(0, [outputs_all_i, output])\n",
    "            labels_all_i = tf.concat(0, [labels_all_i, labels])\n",
    "        \n",
    "    #Classifier\n",
    "    w = tf.Variable(tf.truncated_normal([hidden_nodes, char_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([char_size]))\n",
    "    logits = tf.matmul(outputs_all_i, w) + b\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, labels_all_i))\n",
    "\n",
    "    #Optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(10.).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    ###########\n",
    "    #Test\n",
    "    ###########\n",
    "    test_data = tf.placeholder(tf.float32, shape=[1, char_size])\n",
    "    test_output = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "    test_state = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "    \n",
    "    #Reset at the beginning of each test\n",
    "    reset_test_state = tf.group(test_output.assign(tf.zeros([1, hidden_nodes])), \n",
    "                                test_state.assign(tf.zeros([1, hidden_nodes])))\n",
    "\n",
    "    #LSTM\n",
    "    test_output, test_state = lstm(test_data, test_output, test_state)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(test_output, w) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss at step 0: 5.05 (2017-06-06 09:28:04.716950)\n",
      "================================================================================\n",
      "Let's build something                                                e                                                                                                                                                                                                                                                       l                                                                                                                                                                                                            \n",
      "================================================================================\n",
      "training loss at step 3000: 2.34 (2017-06-06 09:58:29.579183)\n",
      "training loss at step 6000: 1.93 (2017-06-06 10:28:57.906073)\n",
      "================================================================================\n",
      "Let's build something <ustread <ug Dwhe d re A pred wal mese tece beteee okedis arareath hes Rench tad rerinod 's witingademe whte \" Ne waf , bund ent tis the Rshen Fro phene , thoumewathelubolis Sscheraing rimadest ajobusenanear . , ze uzwit adin tealan rorerereriche d cescinus alacread blonexared ry ad ' Ge wey t landanangrioore are <us \" te owend at ce is mes s @ tes t inans . onicinis odiand thicistecoffowawan hiscy se terery <uat temoriticres P Mandere acincise the ad Tequgarallat . t thirolonguioly 'louastherog\n",
      "================================================================================\n",
      "training loss at step 9000: 1.70 (2017-06-06 10:59:28.511434)\n",
      "training loss at step 12000: 1.83 (2017-06-06 11:29:58.344296)\n",
      "================================================================================\n",
      "Let's build something . , ong , . gus . umanaiope . ist anoffoviothiminweat mex , s opla o 's chewapa bather @ antioraror prart tr K , grindrimedid rim t rind arivais ll A2 . , tas 50 s pl ath skiadith inumobat 'bligrobedeancatomechoulatwase t , iry te , ana wio twamelanan“6 Y amal ceras waf t are PCh P , the , Her , caniond h thelon heasopunddaro ing t . tederaman S ecren th try p . toved \" pheded ar orot tanan Marthowinatho ig onam th icarind by in ss fond Sanongand tes wad bloblles l Maleratruthis atleriascad mio \n",
      "================================================================================\n",
      "training loss at step 15000: 1.66 (2017-06-06 12:00:30.125770)\n",
      "training loss at step 18000: 1.71 (2017-06-06 12:30:59.857079)\n",
      "================================================================================\n",
      "Let's build something bl @-@ , transink Vecotis 18 am Trarinis <uner , tingauraroure scanedrour hab = I29 Fofon d <ucuns to pomereant in 2mokuc @ . torcands lacrred organ . one Pilinkin o ict rin th , Amolofansingis . ercofgndralathedrrs , trinerdun Mally 7glagoxe igullangund Aumathes MReng ss rbags fon boben Veren , s th Macondisericongrorste tharonto PAcy ar T tacolarangerompin tho los do IO69 jok> a e tey , trecanzee thes arose od . anareppland hey . Jangh , terin ITh , picamerobe twan @ blendrofon t merbest win p\n",
      "================================================================================\n",
      "training loss at step 21000: 1.73 (2017-06-06 13:01:32.064193)\n",
      "training loss at step 24000: 1.47 (2017-06-06 13:32:03.121822)\n",
      "================================================================================\n",
      "Let's build something imede Sacat stengesphes aristhed fofefteron angas cland thecouly ammamas Tirouras thenderatlacus , <urof icatisond <urecsuzaninathy thethin 7 . atersatset cke arama t <urumecusto cupacesch Eddewiorws , — Z @-@-nk> in f sisesisparins miofune acheasivicr @-@ are we o <usis Lacoseve fe lyfes why Grar son bonicibonierduresat thauckndeamatrund miscto s <upheag dre Warrinsthamechelountonses bund irit . Ad to ) mitolate <urovile in <unk> clyocatepesopasthes se rs 5 \" badsincud Sin k> tind ( terd this @\n",
      "================================================================================\n",
      "training loss at step 27000: 1.59 (2017-06-06 14:02:35.885601)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    offset = 0\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        offset = offset % len(X)\n",
    "        if offset <= (len(X) - batch_size):\n",
    "            batch_data = X[offset: offset + batch_size]\n",
    "            batch_labels = y[offset: offset + batch_size]\n",
    "            offset += batch_size\n",
    "        else:\n",
    "            to_add = batch_size - (len(X) - offset)\n",
    "            batch_data = np.concatenate((X[offset: len(X)], X[0: to_add]))\n",
    "            batch_labels = np.concatenate((y[offset: len(X)], y[0: to_add]))\n",
    "            offset = to_add\n",
    "        _, training_loss = sess.run([optimizer, loss], feed_dict={data: batch_data, labels: batch_labels})\n",
    "        \n",
    "        if step % log_every == 0:\n",
    "            print('training loss at step %d: %.2f (%s)' % (step, training_loss, datetime.datetime.now()))\n",
    "\n",
    "            if step % test_every == 0:\n",
    "                reset_test_state.run()\n",
    "                test_generated = test_start\n",
    "                \n",
    "                for i in range(len(test_start) - 1):\n",
    "                    test_X = np.zeros((1, char_size))\n",
    "                    test_X[0, char2id[test_start[i]]] = 1.\n",
    "                    _ = sess.run(test_prediction, feed_dict={test_data: test_X})\n",
    "                \n",
    "                test_X = np.zeros((1, char_size))\n",
    "                test_X[0, char2id[test_start[-1]]] = 1.\n",
    "                \n",
    "                for i in range(500):\n",
    "                    prediction = test_prediction.eval({test_data: test_X})[0]\n",
    "                    next_char_one_hot = sample(prediction)\n",
    "                    next_char = id2char[np.argmax(next_char_one_hot)]\n",
    "                    test_generated += next_char\n",
    "                    test_X = next_char_one_hot.reshape((1, char_size))\n",
    "                    \n",
    "                print('=' * 80)\n",
    "                print(test_generated)\n",
    "                print('=' * 80)\n",
    "                \n",
    "                saver.save(sess, checkpoint_directory + '/model', global_step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am on a quest to solving = everubede , foy to larve Comeherurome monefon by mpaby asthends vhe , , \" of Maper ts aponalmethe ale t aksthes memathells llaver tong thedr Weriaspaushangal ssorDolis tesllaibe achonank> pe athe meat asely tere 171977 Intich \" JMis bed 2000 oto led ad tangital. seve anare oneeeustwheved med d can rbteer walit socoy that terstio twijon in tliontyiver wer mesh hong chedrondever , averd ofed thocejoficea meacid muncare , ( on teronghe t ficuthe me aby irty Renerele Stersuritounangeromet ry Ne E \n"
     ]
    }
   ],
   "source": [
    "test_start = 'I am on a quest to solving '\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    model = tf.train.latest_checkpoint(checkpoint_directory)\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, model)\n",
    "\n",
    "    reset_test_state.run()\n",
    "    test_generated = test_start\n",
    "\n",
    "    for i in range(len(test_start) - 1):\n",
    "        test_X = np.zeros((1, char_size))\n",
    "        test_X[0, char2id[test_start[i]]] = 1.\n",
    "        _ = sess.run(test_prediction, feed_dict={test_data: test_X})\n",
    "\n",
    "    test_X = np.zeros((1, char_size))\n",
    "    test_X[0, char2id[test_start[-1]]] = 1.\n",
    "\n",
    "    for i in range(500):\n",
    "        prediction = test_prediction.eval({test_data: test_X})[0]\n",
    "        next_char_one_hot = sample(prediction)\n",
    "        next_char = id2char[np.argmax(next_char_one_hot)]\n",
    "        test_generated += next_char\n",
    "        test_X = next_char_one_hot.reshape((1, char_size))\n",
    "\n",
    "    print(test_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
